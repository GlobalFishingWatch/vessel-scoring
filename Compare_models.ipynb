{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models\n",
    "\n",
    "This compares several different models trained in turn on the *longliner*,\n",
    "*trawler* and *purse seiner* data sets.\n",
    "\n",
    "**NOTE: this was somewhat unstable. Running multiple times yielded significantly different results, \n",
    "  depending the data split.  I set seeds everywhere and now the output is stable. However, this tells \n",
    "  me that we want more data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import vessel_scoring.data\n",
    "from vessel_scoring.evaluate_model import evaluate_model, train_model, compare_auc\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vessel_scoring.legacy_heuristic_model import LegacyHeuristicModel\n",
    "from vessel_scoring.random_forest_model import RandomForestModel\n",
    "from vessel_scoring.logistic_model import LogisticModel\n",
    "\n",
    "untrained_models = [\n",
    "    ('Logistic', LogisticModel(windows=[43200], order=6)),\n",
    "    ('Logistic (MW)', LogisticModel(windows=[1800, 3600, 10800, 21600, 43200, 86400], order=6)),\n",
    "    ('Logistic (MW/cross)', LogisticModel(windows=[1800, 3600, 10800, 21600, 43200, 86400], order=6, cross=2)),\n",
    "    ('Random Forest', RandomForestModel(windows=[43200])),\n",
    "    ('Random Forest (MW)', RandomForestModel(windows=[1800, 3600, 10800, 21600, 43200, 86400])),\n",
    "    ('Legacy', LegacyHeuristicModel(window=3600)),\n",
    "    (\"Legacy (3 Hour)\", LegacyHeuristicModel(window=10800)),\n",
    "    (\"Legacy (12 Hour)\", LegacyHeuristicModel(window=43200)),\n",
    "    (\"Legacy (24 Hour)\", LegacyHeuristicModel(window=86400)),  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Comparison for longliner</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for vessel_class in [\"longliner\", \"trawl\", \"ps\"]:\n",
    "    display(HTML(\"<h1>Comparison for {0}</h1>\".format(vessel_class)))\n",
    "    x, xtrain, xcross, xtest = data.load_dataset_by_vessel(\n",
    "            'datasets/kristina_{0}.measures.npz'.format(vessel_class))\n",
    "    trained_models = [(name, train_model(mdl, xtrain)) for (name, mdl) in untrained_models]\n",
    "    for name, mdl in trained_models:\n",
    "        evaluate_model(mdl, xtest, name=name)\n",
    "    print\n",
    "    print\n",
    "    display(HTML(\"<h1>AUC comparison for {0}</h1>\".format(vessel_class)))\n",
    "    compare_auc(trained_models, xtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
